# Comparing `tmp/aiavatar-0.5.5-py3-none-any.whl.zip` & `tmp/aiavatar-0.5.6-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,33 +1,35 @@
-Zip file size: 38178 bytes, number of entries: 31
+Zip file size: 43702 bytes, number of entries: 33
 -rw-r--r--  2.0 unx      712 b- defN 24-May-12 07:19 aiavatar/__init__.py
 -rw-r--r--  2.0 unx     3407 b- defN 24-May-12 13:20 aiavatar/avatar.py
--rw-r--r--  2.0 unx     8161 b- defN 24-May-12 14:03 aiavatar/bot.py
+-rw-r--r--  2.0 unx     8161 b- defN 24-May-19 01:20 aiavatar/bot.py
 -rw-r--r--  2.0 unx     2196 b- defN 24-May-12 06:14 aiavatar/animation/__init__.py
 -rw-r--r--  2.0 unx     1535 b- defN 24-May-12 06:14 aiavatar/animation/vrchat.py
 -rw-r--r--  2.0 unx        0 b- defN 24-May-05 04:55 aiavatar/api/__init__.py
--rw-r--r--  2.0 unx     6114 b- defN 24-May-12 06:14 aiavatar/api/router.py
--rw-r--r--  2.0 unx     2502 b- defN 24-May-12 06:14 aiavatar/api/schema.py
+-rw-r--r--  2.0 unx    10425 b- defN 24-May-15 15:19 aiavatar/api/router.py
+-rw-r--r--  2.0 unx     8325 b- defN 24-May-15 15:19 aiavatar/api/schema.py
 -rw-r--r--  2.0 unx       31 b- defN 24-Apr-14 09:42 aiavatar/device/__init__.py
 -rw-r--r--  2.0 unx     3790 b- defN 24-Apr-20 03:57 aiavatar/device/audio.py
 -rw-r--r--  2.0 unx     2052 b- defN 24-May-12 06:14 aiavatar/face/__init__.py
 -rw-r--r--  2.0 unx     1709 b- defN 24-May-12 06:14 aiavatar/face/vrchat.py
--rw-r--r--  2.0 unx     8322 b- defN 24-May-12 04:21 aiavatar/listeners/__init__.py
+-rw-r--r--  2.0 unx     8372 b- defN 24-May-19 01:56 aiavatar/listeners/__init__.py
 -rw-r--r--  2.0 unx     2773 b- defN 24-May-05 04:46 aiavatar/listeners/azurevoicerequest.py
 -rw-r--r--  2.0 unx     4951 b- defN 24-May-05 04:46 aiavatar/listeners/azurewakeword.py
+-rw-r--r--  2.0 unx     1655 b- defN 24-May-13 17:33 aiavatar/listeners/openailisteners.py
 -rw-r--r--  2.0 unx      976 b- defN 24-May-12 04:21 aiavatar/listeners/voicerequest.py
 -rw-r--r--  2.0 unx     1132 b- defN 24-May-12 04:21 aiavatar/listeners/wakeword.py
 -rw-r--r--  2.0 unx      178 b- defN 24-Apr-14 09:42 aiavatar/processors/__init__.py
--rw-r--r--  2.0 unx     6490 b- defN 24-May-12 06:53 aiavatar/processors/chatgpt.py
+-rw-r--r--  2.0 unx     9911 b- defN 24-May-19 14:51 aiavatar/processors/chatgpt.py
 -rw-r--r--  2.0 unx     3110 b- defN 24-May-05 11:34 aiavatar/processors/claude.py
--rw-r--r--  2.0 unx     3388 b- defN 24-May-05 11:43 aiavatar/processors/gemini.py
--rw-r--r--  2.0 unx     2590 b- defN 24-May-12 14:03 aiavatar/speech/__init__.py
+-rw-r--r--  2.0 unx     3388 b- defN 24-May-19 14:52 aiavatar/processors/gemini.py
+-rw-r--r--  2.0 unx     2712 b- defN 24-May-15 15:19 aiavatar/speech/__init__.py
 -rw-r--r--  2.0 unx     1887 b- defN 24-May-05 01:03 aiavatar/speech/azurespeech.py
--rw-r--r--  2.0 unx     1348 b- defN 24-May-12 14:03 aiavatar/speech/voicevox.py
+-rw-r--r--  2.0 unx     1474 b- defN 24-May-13 17:33 aiavatar/speech/openaispeech.py
+-rw-r--r--  2.0 unx     1348 b- defN 24-May-13 13:48 aiavatar/speech/voicevox.py
 -rw-r--r--  2.0 unx      220 b- defN 24-Apr-16 15:07 aiavatar/speech/soundplayers/__init__.py
--rw-r--r--  2.0 unx     2145 b- defN 24-Apr-20 04:04 aiavatar/speech/soundplayers/sounddevice_player.py
--rw-r--r--  2.0 unx    11324 b- defN 24-May-12 14:03 aiavatar-0.5.5.dist-info/LICENSE
--rw-r--r--  2.0 unx    25214 b- defN 24-May-12 14:03 aiavatar-0.5.5.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-May-12 14:03 aiavatar-0.5.5.dist-info/WHEEL
--rw-r--r--  2.0 unx        9 b- defN 24-May-12 14:03 aiavatar-0.5.5.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     2604 b- defN 24-May-12 14:03 aiavatar-0.5.5.dist-info/RECORD
-31 files, 110962 bytes uncompressed, 33994 bytes compressed:  69.4%
+-rw-r--r--  2.0 unx     2175 b- defN 24-May-19 14:54 aiavatar/speech/soundplayers/sounddevice_player.py
+-rw-r--r--  2.0 unx    11324 b- defN 24-May-19 14:55 aiavatar-0.5.6.dist-info/LICENSE
+-rw-r--r--  2.0 unx    27979 b- defN 24-May-19 14:55 aiavatar-0.5.6.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-19 14:55 aiavatar-0.5.6.dist-info/WHEEL
+-rw-r--r--  2.0 unx        9 b- defN 24-May-19 14:55 aiavatar-0.5.6.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2787 b- defN 24-May-19 14:55 aiavatar-0.5.6.dist-info/RECORD
+33 files, 130796 bytes uncompressed, 39230 bytes compressed:  70.0%
```

## zipnote {}

```diff
@@ -39,14 +39,17 @@
 
 Filename: aiavatar/listeners/azurevoicerequest.py
 Comment: 
 
 Filename: aiavatar/listeners/azurewakeword.py
 Comment: 
 
+Filename: aiavatar/listeners/openailisteners.py
+Comment: 
+
 Filename: aiavatar/listeners/voicerequest.py
 Comment: 
 
 Filename: aiavatar/listeners/wakeword.py
 Comment: 
 
 Filename: aiavatar/processors/__init__.py
@@ -63,32 +66,35 @@
 
 Filename: aiavatar/speech/__init__.py
 Comment: 
 
 Filename: aiavatar/speech/azurespeech.py
 Comment: 
 
+Filename: aiavatar/speech/openaispeech.py
+Comment: 
+
 Filename: aiavatar/speech/voicevox.py
 Comment: 
 
 Filename: aiavatar/speech/soundplayers/__init__.py
 Comment: 
 
 Filename: aiavatar/speech/soundplayers/sounddevice_player.py
 Comment: 
 
-Filename: aiavatar-0.5.5.dist-info/LICENSE
+Filename: aiavatar-0.5.6.dist-info/LICENSE
 Comment: 
 
-Filename: aiavatar-0.5.5.dist-info/METADATA
+Filename: aiavatar-0.5.6.dist-info/METADATA
 Comment: 
 
-Filename: aiavatar-0.5.5.dist-info/WHEEL
+Filename: aiavatar-0.5.6.dist-info/WHEEL
 Comment: 
 
-Filename: aiavatar-0.5.5.dist-info/top_level.txt
+Filename: aiavatar-0.5.6.dist-info/top_level.txt
 Comment: 
 
-Filename: aiavatar-0.5.5.dist-info/RECORD
+Filename: aiavatar-0.5.6.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## aiavatar/api/router.py

```diff
@@ -1,7 +1,8 @@
+import asyncio
 import collections
 import logging
 import traceback
 from fastapi import APIRouter
 from fastapi.responses import Response
 from aiavatar import AIAvatar
 from .schema import *
@@ -81,14 +82,41 @@
             return GetIsSpeakingResponse(is_speaking=aiavatr_app.avatar_controller.speech_controller.is_speaking())
 
         except Exception as ex:
             response.status_code = 500
             return ErrorResponse(error=f"error: {ex}\n{traceback.format_exc()}")
 
 
+    @api_router.post("/avatar/speech/configuration", tags=["Avatar"], name="Update SpeechController configurations")
+    async def avatar_speech_config(request: SpeechConfigRequest, response: Response) -> SpeechConfigResponse:
+        sc = aiavatr_app.avatar_controller.speech_controller
+
+        # Update configuration if specified
+        for field, value in request.model_dump(exclude_unset=True).items():
+            if value is not None and hasattr(sc, field):
+                setattr(sc, field, value)
+
+        # Clear cache after updating configuration
+        sc.clear_cache()
+
+        # Return updated configuration
+        resp = SpeechConfigResponse.from_speech_controller_base(sc)
+        resp.api_key = "************"
+        return resp
+
+
+    @api_router.get("/avatar/speech/configuration", tags=["Avatar"], name="Get current SpeechController configurations")
+    async def get_avatar_speech_config(response: Response) -> SpeechConfigResponse:
+        resp = SpeechConfigResponse.from_speech_controller_base(
+            aiavatr_app.avatar_controller.speech_controller
+        )
+        resp.api_key = "************"
+        return resp
+
+
     @api_router.post("/avatar/face", tags=["Avatar"], name="Set face expression")
     async def avatar_face(request: FaceRequest, response: Response) -> APIResponse:
         try:
             await aiavatr_app.avatar_controller.face_controller.set_face(request.name, request.duration)
             return APIResponse(message="success")
 
         except Exception as ex:
@@ -125,14 +153,84 @@
             return GetAnimationResponse(current_animation=current_animation)
 
         except Exception as ex:
             response.status_code = 500
             return ErrorResponse(error=f"error: {ex}\n{traceback.format_exc()}")
 
 
+    @api_router.post("/processor/chat", tags=["Chat Processor"], name="Send message to ChatProcessor")
+    async def processor_chat(request: ChatRequest, response: Response) -> APIResponse:
+        try:
+            response_text = ""
+            avatar_task = asyncio.create_task(aiavatr_app.avatar_controller.start())
+            stream_buffer = ""
+            async for t in aiavatr_app.chat_processor.chat(request.text):
+                stream_buffer += t
+                for spc in aiavatr_app.split_chars:
+                    stream_buffer = stream_buffer.replace(spc, spc + "|")
+                sp = stream_buffer.split("|")
+                if len(sp) > 1: # >1 means `|` is found (splited at the end of sentence)
+                    sentence = sp.pop(0)
+                    stream_buffer = "".join(sp)
+                    aiavatr_app.avatar_controller.set_text(sentence)
+                    response_text += sentence
+                await asyncio.sleep(0.01)   # wait slightly in every loop not to use up CPU
+
+            if stream_buffer:
+                aiavatr_app.avatar_controller.set_text(stream_buffer)
+                response_text += stream_buffer
+
+            aiavatr_app.avatar_controller.set_stop()
+            await avatar_task
+
+            return APIResponse(message=response_text)
+
+        except Exception as ex:
+            response.status_code = 500
+            return ErrorResponse(error=f"error: {ex}\n{traceback.format_exc()}")
+
+
+    @api_router.get("/processor/histories", tags=["Chat Processor"], name="Get current histories")
+    async def get_processor_histories(response: Response) -> GetHistoriesResponse:
+        return GetHistoriesResponse(
+            histories=aiavatr_app.chat_processor.histories
+        )
+
+
+    @api_router.delete("/processor/histories", tags=["Chat Processor"], name="Delete histories")
+    async def delete_processor_histories(response: Response) -> APIResponse:
+        aiavatr_app.chat_processor.reset_histories()
+        return APIResponse(message="deleted")
+
+
+    @api_router.post("/processor/configuration", tags=["Chat Processor"], name="Update ChatProcessor configurations")
+    async def processor_config(request: ChatProcessorConfigRequest, response: Response) -> ChatProcessorConfigResponse:
+        proc = aiavatr_app.chat_processor
+
+        # Update configuration if specified
+        for field, value in request.model_dump(exclude_unset=True).items():
+            if value is not None and hasattr(proc, field):
+                setattr(proc, field, value)
+
+        # Clear histories after updating configuration
+        proc.histories.clear()
+
+        # Return updated configuration
+        resp = ChatProcessorConfigResponse.from_chat_processor(proc)
+        resp.api_key = "************"
+        return resp
+
+
+    @api_router.get("/processor/configuration", tags=["Chat Processor"], name="Get current ChatProcessor configurations")
+    async def get_processor_config(response: Response) -> ChatProcessorConfigResponse:
+        resp = ChatProcessorConfigResponse.from_chat_processor(aiavatr_app.chat_processor)
+        resp.api_key = "************"
+        return resp
+
+
     @api_router.post("/system/log", tags=["System"], name="See the recent log")
     async def system_log(request: LogRequest, response: Response) -> LogResponse:
         try:
             with open(logfile_path, "r", encoding="utf-8") as f:
                 deque_lines = collections.deque(f, maxlen=request.count)
                 return LogResponse(lines=deque_lines)
```

## aiavatar/api/schema.py

```diff
@@ -1,9 +1,11 @@
 from pydantic import BaseModel, Field
-from typing import List
+from typing import List, Optional, Dict
+import aiavatar
+import aiavatar.speech
 
 
 class APIResponse(BaseModel):
     message: str = Field(..., example="Message from API", description="Message from API")
 
 
 class ErrorResponse(BaseModel):
@@ -12,15 +14,15 @@
 
 class WakewordStartRequest(BaseModel):
     wakewords: List[str] = Field(default=[], description="List of wakewords to start chat.")
 
 
 class WakewordStatusResponse(BaseModel):
     is_listening: bool = Field(..., description="Whether the WakewordListener is listening")
-    thread_name: str|None = Field(None, example="Thread-2", description="The id of the thread wakeword listener is running in")
+    thread_name: Optional[str] = Field(None, example="Thread-2", description="The id of the thread wakeword listener is running in")
 
 
 class GetAvatarStatusResponse(BaseModel):
     is_speaking: bool = Field(..., description="Whether the avatar is speaking")
     current_face: str = Field(..., example="fun", description="Name of current face expression")
     current_animation: str = Field(..., example="wave_hands", description="Name of current animation")
 
@@ -29,14 +31,63 @@
     text: str = Field(..., example="[face:joy]Hi, let's talk with me!", description="Text to speech with face and animation tag")
 
 
 class GetIsSpeakingResponse(BaseModel):
     is_speaking: bool = Field(..., description="Whether the avatar is speaking")
 
 
+class SpeechConfigBase(BaseModel):
+    # Common
+    base_url: Optional[str] = Field(None, example="http://127.0.0.1:50021", description="Base url for speech service")
+    rate: Optional[int] = Field(None, example="16000", description="Sample rate")
+    device_index: Optional[int] = Field(None, example="1", description="Output device index")
+    playback_margin: Optional[float] = Field(None, example="0.1", description="Margin in seconds after playback")
+    use_subprocess: Optional[bool] = Field(None, example=True, description="Enable or disable the use of subprocess for TTS")
+    subprocess_timeout: Optional[float] = Field(None, example="5.0", description="Timeout duration for the subprocess in seconds")
+    # VOICEVOX
+    speaker_id: Optional[int] = Field(None, example="1", description="ID of the speaker to use in VOICEVOX")
+    # OpenAI/Azure
+    api_key: Optional[str] = Field(None, example="sk-xxxxxxxxxxxx", description="API key for accessing the TTS service")
+    # OpenAI
+    voice: Optional[str] = Field(None, example="alloy", description="Voice model to use for OpenAI TTS")
+    model: Optional[str] = Field(None, example="whisper-1", description="Model name for OpenAI TTS")
+    speed: Optional[float] = Field(None, example="1.0", description="Speech speed multiplier")
+    # Azure
+    region: Optional[str] = Field(None, example="japaneast", description="Azure region for TTS service")
+    speaker_name: Optional[str] = Field(None, example="en-US-JennyMultilingualNeural", description="Name of the speaker to use in Azure TTS")
+    speaker_gender: Optional[str] = Field(None, example="Female", description="Gender of the speaker for Azure TTS")
+    lang: Optional[str] = Field(None, example="en-US", description="Language code for TTS")
+
+    @classmethod
+    def from_speech_controller_base(cls, sc: aiavatar.speech.SpeechControllerBase):
+        return cls(
+            base_url=getattr(sc, "base_url", None),
+            rate=getattr(sc, "rate", None),
+            device_index=getattr(sc, "device_index", None),
+            playback_margin=getattr(sc, "playback_margin", None),
+            use_subprocess=getattr(sc, "use_subprocess", None),
+            subprocess_timeout=getattr(sc, "subprocess_timeout", None),
+            speaker_id=getattr(sc, "speaker_id", None),
+            api_key=getattr(sc, "api_key", None),
+            voice=getattr(sc, "voice", None),
+            model=getattr(sc, "model", None),
+            speed=getattr(sc, "speed", None),
+            region=getattr(sc, "region", None),
+            speaker_name=getattr(sc, "speaker_name", None),
+            speaker_gender=getattr(sc, "speaker_gender", None),
+            lang=getattr(sc, "lang", None),
+        )
+
+
+class SpeechConfigRequest(SpeechConfigBase): ...
+
+
+class SpeechConfigResponse(SpeechConfigBase): ...
+
+
 class FaceRequest(BaseModel):
     name: str = Field(..., example="fun", description="Name of face expression to set")
     duration: float = Field(4.0, example=4.0, description="Duration in seconds for how long the face expression should last")
 
 
 class GetFaceResponse(BaseModel):
     current_face: str = Field(..., example="fun", description="Name of current face expression")
@@ -47,13 +98,60 @@
     duration: float = Field(4.0, example=4.0, description="Duration in seconds for how long the animation should last")
 
 
 class GetAnimationResponse(BaseModel):
     current_animation: str = Field(..., example="wave_hands", description="Name of current animation")
 
 
+class ChatRequest(BaseModel):
+    text: str = Field(..., example="„Åì„Çì„Å´„Å°„ÅØÔºÅ", description="Text message to send to ChatProcessor")
+
+
+class GetHistoriesResponse(BaseModel):
+    histories: list[dict] = Field(..., example='[{"role": "user", "content": "„Åì„Çì„Å´„Å°„ÅØ"}, {"role": "assistant", "content": "„Åì„Çì„Å´„Å°„ÅØÔºÅ‰Ωï„Åã„ÅäÊâã‰ºù„ÅÑ„Åô„Çã„Åì„Å®„ÅØ„ÅÇ„Çä„Åæ„Åô„ÅãÔºü"}]', description="Histories cached in ChatProcessor")
+
+
+class ChatProcessorConfig(BaseModel):
+    # Common fields
+    api_key: Optional[str] = Field(None, example="sk-xxxxxxxxxxxx", description="API key for accessing the chat service")
+    model: Optional[str] = Field(None, example="gpt-4o", description="Model name for the chat service")
+    temperature: Optional[float] = Field(None, example=1.0, description="Temperature setting for the chat model")
+    max_tokens: Optional[int] = Field(None, example=200, description="Maximum tokens for the response")
+    functions: Optional[Dict] = Field(None, example={}, description="Functions for the chat model")
+    system_message_content: Optional[str] = Field(None, example="System message content", description="Content of the system message")
+    history_count: Optional[int] = Field(None, example=10, description="Number of history entries to retain")
+    history_timeout: Optional[float] = Field(None, example=60.0, description="Timeout for history entries in seconds")
+    # ChatGPTProcessor
+    base_url: Optional[str] = Field(None, example="http://127.0.0.1:8080", description="Base URL for the chat service")
+    parse_function_call_in_response: Optional[bool] = Field(None, example=True, description="Whether to parse function call in response")
+    # GeminiProcessor
+    system_message_content_acknowledgement_content: Optional[str] = Field(None, example="‰∫ÜËß£„Åó„Åæ„Åó„Åü„ÄÇ", description="Acknowledgement content for system message")
+
+    @classmethod
+    def from_chat_processor(cls, processor) -> "ChatProcessorConfig":
+        return cls(
+            api_key=getattr(processor, "api_key", None),
+            base_url=getattr(processor, "base_url", None),
+            model=getattr(processor, "model", None),
+            temperature=getattr(processor, "temperature", None),
+            max_tokens=getattr(processor, "max_tokens", None),
+            functions=getattr(processor, "functions", None),
+            parse_function_call_in_response=getattr(processor, "parse_function_call_in_response", None),
+            system_message_content=getattr(processor, "system_message_content", None),
+            system_message_content_acknowledgement_content=getattr(processor, "system_message_content_acknowledgement_content", None),
+            history_count=getattr(processor, "history_count", None),
+            history_timeout=getattr(processor, "history_timeout", None),
+        )
+
+
+class ChatProcessorConfigRequest(ChatProcessorConfig): ...
+
+
+class ChatProcessorConfigResponse(ChatProcessorConfig): ...
+
+
 class LogRequest(BaseModel):
     count: int = Field(50, example=50, description="Lines from tail to read")
 
 
 class LogResponse(BaseModel):
     lines: List[str] = Field(default=[], examples=["[INFO] 2024-05-05 00:25:08,070 : AzureWakeWordListener: Hello", "[INFO] 2024-05-05 00:25:13,949 : User: Hello", "[INFO] 2024-05-05 00:25:13,949 : AI: Hello! What's up?"], description="List of lines in log file")
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## aiavatar/listeners/__init__.py

```diff
@@ -34,15 +34,15 @@
         self.channels = channels
         self.rate = rate
         self.device_index = device_index
 
     def get_volume_db(self, data: numpy.ndarray[numpy.int16], ref: int=32768) -> float:
         amplitude = numpy.max(numpy.abs(data))
         if amplitude == 0:
-            return -numpy.inf
+            amplitude = 1   # Return 1 to calculate dB
         return float(20 * numpy.log10(amplitude / ref))
 
     def get_noise_level(self) -> float:
         with sounddevice.InputStream(
                 device=self.device_index,
                 channels=self.channels,
                 samplerate=self.rate,
@@ -85,15 +85,15 @@
         self.rate = rate
         self.device_index = device_index
         self.is_listening = False
 
     def get_volume_db(self, data: numpy.ndarray[numpy.int16], ref: int=32768) -> float:
         amplitude = numpy.max(numpy.abs(data))
         if amplitude == 0:
-            return -numpy.inf
+            amplitude = 1   # Return 1 to calculate dB
         return float(20 * numpy.log10(amplitude / ref))
 
     def record_audio(self, device_index) -> bytes:
         audio_data = []
 
         try:
             stream = sounddevice.InputStream(
```

## aiavatar/processors/chatgpt.py

```diff
@@ -1,7 +1,9 @@
+from abc import abstractmethod
+import base64
 from logging import getLogger, NullHandler
 from datetime import datetime
 import traceback
 import json
 from typing import Iterator, Callable, AsyncGenerator
 from openai import AsyncClient
 from . import ChatProcessor
@@ -52,15 +54,15 @@
 
     def add_function(self, name: str, description: str=None, parameters: dict=None, func: Callable=None):
         self.functions[name] = ChatGPTFunction(name=name, description=description, parameters=parameters, func=func)
 
     def reset_histories(self):
         self.histories.clear()
 
-    def build_messages(self, text):
+    async def build_messages(self, text):
         messages = []
         try:
             # System message
             if self.system_message_content:
                 messages.append({"role": "system", "content": self.system_message_content})
 
             # Histories
@@ -107,15 +109,15 @@
 
             if (datetime.utcnow() - self.last_chat_at).total_seconds() > self.history_timeout:
                 self.reset_histories()
 
             if self.on_start_processing:
                 await self.on_start_processing()
 
-            messages = self.build_messages(text)
+            messages = await self.build_messages(text)
 
             response_text = ""
             stream_resp = await self.chat_completion_stream(async_client, messages)
 
             async for chunk in stream_resp.stream:
                 delta = chunk.choices[0].delta
                 if stream_resp.response_type == "content":
@@ -163,7 +165,73 @@
             self.logger.error(f"Error at chat: {str(ex)}\n{traceback.format_exc()}")
             raise ex
         
         finally:
             self.last_chat_at = datetime.utcnow()
             if not async_client.is_closed():
                 await async_client.close()
+
+
+class ChatGPTProcessorWithVisionBase(ChatGPTProcessor):
+    def __init__(self, *, api_key: str, base_url: str = None, model: str = "gpt-3.5-turbo", temperature: float = 1, max_tokens: int = 0, functions: dict = None, parse_function_call_in_response: bool = True, system_message_content: str = None, history_count: int = 10, history_timeout: float = 60, use_vision: bool = True):
+        super().__init__(api_key=api_key, base_url=base_url, model=model, temperature=temperature, max_tokens=max_tokens, functions=functions, parse_function_call_in_response=parse_function_call_in_response, system_message_content=system_message_content, history_count=history_count, history_timeout=history_timeout)
+        self.use_vision = use_vision
+
+    @abstractmethod
+    async def get_image(self) -> bytes:
+        pass
+
+    async def build_messages(self, text):
+        messages = await super().build_messages(text)
+        if not self.use_vision:
+            return messages
+
+        if len(self.histories) > 1:
+            last_user_message = self.histories[-2:-1][0]
+            if isinstance(last_user_message["content"], list):
+                for i in range(len(last_user_message["content"]) - 1, -1, -1):
+                    # Remove image from last request
+                    if last_user_message["content"][i].get("type") != "text":
+                        del last_user_message["content"][i]
+
+        async_client = AsyncClient(api_key=self.api_key)
+        try:
+            # Determine whether the vision input is required to process the user input
+            resp = await async_client.chat.completions.create(
+                model=self.model,
+                messages=messages[:-1] + [{"role": "user", "content": f"‰ª•‰∏ã„ÅØ„É¶„Éº„Ç∂„Éº„Åã„Çâ„ÅÆÂÖ•ÂäõÂÜÖÂÆπ„Åß„Åô„ÄÇ„Åì„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíÂá¶ÁêÜ„Åô„Çã„ÅÆ„Å´Êñ∞„Åü„Å™ÁîªÂÉè„ÅÆÂÖ•Âäõ„ÅåÂøÖË¶Å„ÅãÂà§Êñ≠„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\nÂÖ•Âäõ: {text}"}],
+                functions=[{
+                    "name": "is_vision_required",
+                    "description": "Determine whether the vision input is required to process the user input.",
+                    "parameters": {
+                        "type": "object",
+                        "properties": {
+                            "is_required": {"type": "boolean"}
+                        },
+                        "required": ["is_required"]
+                    }
+                }],
+                function_call={"name": "is_vision_required"},
+                stream=False
+            )
+
+            if "true" in resp.choices[0].message.function_call.arguments:
+                self.logger.info("Vision input is required")
+
+                # Convert image to data url
+                image_bytes = await self.get_image()
+                image_b64 = base64.b64encode(image_bytes).decode("utf-8")
+                image_data_url = f"data:image/png;base64,{image_b64}"
+                # Overwrite content
+                messages[-1]["content"] = [
+                    {"type": "text", "text": text},
+                    {"type": "image_url", "image_url": {"url": image_data_url}}
+                ]
+
+        except Exception as ex:
+            self.logger.error(f"Error at build_messages: {str(ex)}\n{traceback.format_exc()}")
+
+        finally:
+            if not async_client.is_closed():
+                await async_client.close()
+
+        return messages
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## aiavatar/speech/__init__.py

```diff
@@ -12,14 +12,18 @@
         pass
 
     @abstractmethod
     async def speak(self, text: str):
         pass
 
     @abstractmethod
+    def clear_cache(self):
+        pass
+
+    @abstractmethod
     def is_speaking(self) -> bool:
         pass
 
 
 class VoiceClip:
     def __init__(self, text: str):
         self.text = text
@@ -80,9 +84,12 @@
 
         except Exception as ex:
             self.logger.error(f"Error at speaking: {str(ex)}\n{traceback.format_exc()}")
 
         finally:
             self._is_speaking = False
 
+    def clear_cache(self):
+        self.voice_clips.clear()
+
     def is_speaking(self) -> bool:
         return self._is_speaking
```

## aiavatar/speech/soundplayers/sounddevice_player.py

```diff
@@ -22,15 +22,15 @@
                 dtype=numpy.int16
             )
             framerate = f.getframerate()
             sounddevice.play(data, framerate, device=device_index)
             sounddevice.wait()
     
     except Exception as ex:
-        sys.stderr.write(f"{ex}\n")
+        sys.stderr.write(f"{ex}\ndevice_index: {device_index}\n")
         sys.exit(1)
 
 else:
     from . import SoundPlayerBase
 
     class SoundDevicePlayer(SoundPlayerBase):
         def __init__(self, device_index: int=-1, playback_margin: float=0.1, subprocess_timeout: float=5.0):
```

## Comparing `aiavatar-0.5.5.dist-info/LICENSE` & `aiavatar-0.5.6.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `aiavatar-0.5.5.dist-info/METADATA` & `aiavatar-0.5.6.dist-info/METADATA`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: aiavatar
-Version: 0.5.5
+Version: 0.5.6
 Summary: ü•∞ Building AI-based conversational avatars lightning fast ‚ö°Ô∏èüí¨
 Home-page: https://github.com/uezo/aiavatar
 Author: uezo
 Author-email: uezo@uezo.net
 Maintainer: uezo
 Maintainer-email: uezo@uezo.net
 License: Apache v2
@@ -91,14 +91,15 @@
   - [üé≠ Custom Behavior](#-custom-behavior)
 - [üåé Platform Guide](#-platform-guide)
   - [üêà VRChat](#-vrchat)
   - [üçì Raspberry Pi](#-raspberry-pi)
 - [üß© RESTful APIs](#-restful-apis)
 - [ü§ø Deep Dive](#-deep-dive)
   - [‚ö°Ô∏è Function Calling](#Ô∏è-function-calling)
+  - [üëÄ Vision](#Ô∏è-vision)
 - [üîç Other Tips](#-other-tips)
   - [üé§ Testing Audio I/O](#-testing-audio-io)
   - [üéöÔ∏è Noise Filter](#-noise-filter)
   - [üß™ LM Studio API](#-lm-studio-api)
   - [‚ö°Ô∏è Use Custom Listener](#Ô∏è-use-custom-listener)
 
 
@@ -370,14 +371,67 @@
     YOUR_REGION_NAME,
     on_wakeword=on_wakeword,
     wakewords=["Hello", "„Åì„Çì„Å´„Å°„ÅØ"],
     device_name="BuiltInMicrophoneDevice"
 )
 ```
 
+
+## üç• Using OpenAI's audio APIs
+
+OpenAI's Speech-to-Text and Text-to-Speech capabilities provide dynamic speech recognition and voice output across multiple languages, without the need for fixed language settings.
+
+```python
+from aiavatar import AIAvatar
+from aiavatar.device import AudioDevice
+from aiavatar.listeners.openailisteners import (
+    OpenAIWakewordListener,
+    OpenAIVoiceRequestListener
+)
+from aiavatar.speech.openaispeech import OpenAISpeechController
+
+# Get default audio devices
+devices = AudioDevice()
+
+# Speech
+speech_controller = OpenAISpeechController(
+    api_key=OPENAI_API_KEY,
+    device_index=devices.output_device
+)
+
+# Wakeword
+async def on_wakeword(text):
+    await app.start_chat(request_on_start=text, skip_start_voice=True)
+
+wakeword_listener = OpenAIWakewordListener(
+    api_key=OPENAI_API_KEY,
+    device_index=devices.input_device,
+    wakewords=["„Åì„Çì„Å´„Å°„ÅØ"],
+    on_wakeword=on_wakeword
+)
+
+# Request
+request_listener = OpenAIVoiceRequestListener(
+    api_key=OPENAI_API_KEY,
+    device_index=devices.input_device
+)
+
+# Create AIAvatar with OpenAI Components
+app = AIAvatar(
+    openai_api_key=OPENAI_API_KEY,
+    wakeword_listener=wakeword_listener,
+    request_listener=request_listener,
+    speech_controller=speech_controller,
+    noise_margin=10.0,
+    verbose=True
+)
+app.start_listening_wakeword()
+```
+
+
 ## üîà Audio device
 
 You can specify the audio devices to be used in components by name or index.
 
 ```python
 from aiavatar.device import AudioDevice
 
@@ -727,14 +781,53 @@
     "role": "function",
     "content": "{\"weather\": \"sunny partly cloudy\", \"temperature\": 23.4}",
     "name": "get_weather"
 }
 ```
 
 
+## üëÄ Vision
+
+We provide the experimental support for vision input to ChatGPT. A new class, ChatGPTProcessorWithVisionBase, has been added to handle image inputs, inheriting from ChatGPTProcessor.
+
+An example implementation, ChatGPTProcessorWithVisionScreenShot, demonstrates how to capture screenshots using pyautogui. This gives "eyes" to your AIAvatar in metaverse platforms like VRChat.
+
+```python
+import io
+import pyautogui
+
+class ChatGPTProcessorWithVisionScreenShot(ChatGPTProcessorWithVisionBase):
+    async def get_image(self) -> bytes:
+        buffered = io.BytesIO()
+        image = pyautogui.screenshot(region=(0, 0, 1280, 720))
+        image.save(buffered, format="PNG")
+        image.save("image_to_chatgpt.png")
+        return buffered.getvalue()
+```
+
+To use this new feature, you can instantiate ChatGPTProcessorWithVisionScreenShot instead of ChatGPTProcessor and set it in the AIAvatar.
+
+```python
+chat_processor = ChatGPTProcessorWithVisionScreenShot(
+    api_key=OPENAI_API_KEY,
+    system_message_content=PROMPT
+)
+
+app = AIAvatar(
+    google_api_key=GOOGLE_API_KEY,
+    chat_processor=chat_processor
+)
+```
+
+**NOTE**
+
+* Only the latest image will be sent to ChatGPT to avoid performance issues.
+* The system uses function calling to determine if image retrieval is necessary, which adds approximately 500 milliseconds to 1 second to the processing time.
+
+
 # üîç Other Tips
 
 Useful information for developping and debugging.
 
 ## üé§ Testing audio I/O
 
 Using the script below to test the audio I/O before configuring AIAvatar.
```

## Comparing `aiavatar-0.5.5.dist-info/RECORD` & `aiavatar-0.5.6.dist-info/RECORD`

 * *Files 13% similar despite different names*

```diff
@@ -1,31 +1,33 @@
 aiavatar/__init__.py,sha256=o_MsrBgCxbfFE8FeUCeDJpAtWcUmd0iopx9yPbQkUMg,712
 aiavatar/avatar.py,sha256=ML4A_-cFggDOT4Sxb7VQMHvHD_rATxhY3Pt85ZppEKg,3407
 aiavatar/bot.py,sha256=8L23QrCvCQUxin4i1W-RLRiuTrOFQh3e5fJtzdNATu8,8161
 aiavatar/animation/__init__.py,sha256=_HEhMWleYPmPZFhqfqLVx57S0XXHDBTY_wZgNtDQLbc,2196
 aiavatar/animation/vrchat.py,sha256=KOc4zLdqR7aS69ptDVGGitdeWvKxmku_aoN31pjQSQg,1535
 aiavatar/api/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-aiavatar/api/router.py,sha256=otvzBp_BEmq-x3GgE5rmF8NX10ghOjz9LjbA_fWXS2g,6114
-aiavatar/api/schema.py,sha256=UE5Oi03LJzcivZIB_4EqLdtf47BrigOKUoMmqRABjIw,2502
+aiavatar/api/router.py,sha256=E_mFeP_FGS0DdXF0ixKZlRVPkgThDPv1uXn5AR22Bcc,10425
+aiavatar/api/schema.py,sha256=1qvY8-6o5VFhXtaKzn_Mn2xhgz1X49QMerSI2PUmppg,8325
 aiavatar/device/__init__.py,sha256=4DhskQxS20PcErkyF3z5-RuvXtGCQvw37jqB-yc2As8,31
 aiavatar/device/audio.py,sha256=Y_91icGtl0X6mVuder1t_PGAauBryKBhFl9goiKS6KM,3790
 aiavatar/face/__init__.py,sha256=gwrBXrUGbYPofSCUOfWU-dyXDJ20TVkXkRFLkwZWTs0,2052
 aiavatar/face/vrchat.py,sha256=lG_S7QEG-tK7yHpT4qk1sGKZwD7tiRYDfs8AsMtGkoI,1709
-aiavatar/listeners/__init__.py,sha256=uCNw86Q1ctwDB9vX-ypZ1CnSOLpss2DIb0PMITAk5jg,8322
+aiavatar/listeners/__init__.py,sha256=kVfSeQDkXjmU_8VIVbAEH3RMiXBZu7WpA3UuH4joe3o,8372
 aiavatar/listeners/azurevoicerequest.py,sha256=3DHxZl9uPLEvVH2c1gpbBmwUcVFlPaOzyO35JrIXyoc,2773
 aiavatar/listeners/azurewakeword.py,sha256=2GP_bUi56oFjARXGKFHES_KztA_MBNdiXkbYkor1eCQ,4951
+aiavatar/listeners/openailisteners.py,sha256=hSsKDTkCFO80w7xt2Nq_zUHdohM9zbP5BmOKF_H5vBM,1655
 aiavatar/listeners/voicerequest.py,sha256=6h86YcMXUX_yFd5AAMo4zeQ80xhPTUwhSCQzgWd4ZuY,976
 aiavatar/listeners/wakeword.py,sha256=ZEGGKn5gmNZ8woSUKEbgLJX7o5iR3JD1rG3rRhoehoY,1132
 aiavatar/processors/__init__.py,sha256=k6qF1_UopWpaxQ89OxP7-dSVLgnrDCuuZH0Gom0JLLU,178
-aiavatar/processors/chatgpt.py,sha256=bUx7PqP6yvZno5-Q-m2Eh4ojnIiMgsBl8x1hvVI5KDQ,6490
+aiavatar/processors/chatgpt.py,sha256=L5-DoXHdZ-BUro7af0yXT4i5BF-6En7Ch0meXuEkPSA,9911
 aiavatar/processors/claude.py,sha256=c56BtK4Qsi-I4tQw0J_Jvflgu9bNXpCVV-EMqZEMhV0,3110
 aiavatar/processors/gemini.py,sha256=9cRMEpxFHsoxABysoPch-CPMUkRdD0-gO5AaqxRsuYk,3388
-aiavatar/speech/__init__.py,sha256=LdLDLZlLXUNAWXPHGsq-qMIXf5K2MhOtUDh5p-i-9Ho,2590
+aiavatar/speech/__init__.py,sha256=MugrpbtCQGpDOlBegVFZjuBMxNAx3azEsjoC8XsU9Ss,2712
 aiavatar/speech/azurespeech.py,sha256=P8lnUMj4j8BRnasxCxCb-xkkFA13BvHUG03aZ04RXNU,1887
+aiavatar/speech/openaispeech.py,sha256=IHVdK2o2yh7upZtu-JkQiSz2_OOdofjzzLRYbLSXSl0,1474
 aiavatar/speech/voicevox.py,sha256=HFrtZjE6Q8YNot-WTVEH6kTw8vpG5QxCvK2NUy1WTG4,1348
 aiavatar/speech/soundplayers/__init__.py,sha256=71xYi-8qaInz_Um8wV5zDaHpAB5sTJG9osn17aQAUKQ,220
-aiavatar/speech/soundplayers/sounddevice_player.py,sha256=tBHMJ5eOiM_rKkvCWlBPE0254Ro4lcNkq8XWLB2tHP8,2145
-aiavatar-0.5.5.dist-info/LICENSE,sha256=UOZ1F5fFDe3XXvG4oNnkL1-Ecun7zpHzRxjp-XsMeAo,11324
-aiavatar-0.5.5.dist-info/METADATA,sha256=pdeIhzcr9epJe5OcDu6UShWRIkZOe4RtX2g5PZQQT5U,25214
-aiavatar-0.5.5.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-aiavatar-0.5.5.dist-info/top_level.txt,sha256=B14WVaakM_kKuOkCeI-WRP83BJ6APkOyQrn6EXxs8kg,9
-aiavatar-0.5.5.dist-info/RECORD,,
+aiavatar/speech/soundplayers/sounddevice_player.py,sha256=dDMdlSVdgHFiDNiGxezTon3pLnm8vGia_ftX0URKEm0,2175
+aiavatar-0.5.6.dist-info/LICENSE,sha256=UOZ1F5fFDe3XXvG4oNnkL1-Ecun7zpHzRxjp-XsMeAo,11324
+aiavatar-0.5.6.dist-info/METADATA,sha256=5oP2uioH61UdQBN0UPCvAXGgdoP4hi5KyQludPxNAvY,27979
+aiavatar-0.5.6.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+aiavatar-0.5.6.dist-info/top_level.txt,sha256=B14WVaakM_kKuOkCeI-WRP83BJ6APkOyQrn6EXxs8kg,9
+aiavatar-0.5.6.dist-info/RECORD,,
```

